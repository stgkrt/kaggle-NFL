{"cells":[{"cell_type":"markdown","metadata":{},"source":["# NFL EDA"]},{"cell_type":"markdown","metadata":{},"source":["# import libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-12-11T13:46:28.362881Z","iopub.status.busy":"2022-12-11T13:46:28.362256Z","iopub.status.idle":"2022-12-11T13:46:28.591708Z","shell.execute_reply":"2022-12-11T13:46:28.590982Z","shell.execute_reply.started":"2022-12-11T13:46:28.362786Z"},"trusted":true},"outputs":[],"source":["# general\n","import os\n","import gc\n","import pickle\n","import glob\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","# deep learning\n","import timm\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import SGD, Adam, AdamW\n","from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# loss metrics\n","from sklearn.metrics import matthews_corrcoef\n"," \n","# from torchmetrics.classification import BinaryMatthewsCorrCoef\n","# matthews_corrcoef = BinaryMatthewsCorrCoef()\n","\n","# from torchmetrics import MatthewsCorrCoef\n","# matthews_corrcoef = MatthewsCorrCoef(num_classes=1)#default threshold=0.5\n","\n","# warningの表示方法の設定\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Set Configurations"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T13:46:28.594555Z","iopub.status.busy":"2022-12-11T13:46:28.593848Z","iopub.status.idle":"2022-12-11T13:46:28.60004Z","shell.execute_reply":"2022-12-11T13:46:28.599192Z","shell.execute_reply.started":"2022-12-11T13:46:28.594502Z"},"trusted":true},"outputs":[],"source":["kaggle = False\n","DEBUG = True\n","class CFG:\n","    if kaggle:\n","        BASE_DIR = \"/kaggle/input/nfl-player-contact-detection\"\n","    else:\n","        BASE_DIR = \"/workspace/input\"\n","    TRAIN_HELMET_CSV = os.path.join(BASE_DIR, \"train_baseline_helmets.csv\")\n","    TRAIN_TRACKING_CSV = os.path.join(BASE_DIR, \"train_player_tracking.csv\")\n","    TRAIN_VIDEO_META_CSV = os.path.join(BASE_DIR, \"train_video_metadata.csv\")\n","    TRAIN_LABEL_CSV = os.path.join(BASE_DIR, \"train_labels.csv\")\n","\n","    # data config    \n","    img_size = (224, 224)\n","    batch_size = 256\n","    num_workers = 0\n","    n_fold = 1\n","\n","    # model config\n","    model_name = \"tf_efficientnet_b0\"\n","    out_features = 1\n","    inp_channels= 3\n","    pretrained = True\n","    \n","    # learning config\n","    n_epoch = 20\n","    lr = 1e-6\n","    # max_lr = 2e-5\n","    T_max = 10\n","    min_lr = 1e-7\n","    weight_decay = 1e-6\n","    # opt_wd_non_norm_bias = 0.01\n","    # opt_wd_norm_bias = 0\n","    # opt_beta1 = 0.9\n","    # opt_beta2 = 0.99\n","    # opt_eps = 1e-5\n","    \n","    # validation verbose\n","    epoch_step_valid = 3\n","    steps_per_epoch = 50\n","    \n","    # etc\n","    random_seed = 21\n","    \n","    if DEBUG:\n","        n_epoch = 5"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Utils"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["def seed_everything(seed=CFG.random_seed):\n","    #os.environ['PYTHONSEED'] = str(seed)\n","    np.random.seed(seed%(2**32-1))\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic =True\n","    torch.backends.cudnn.benchmark = False\n","seed_everything()\n","\n","# device optimization\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","print(f'Using device: {device}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Dataset Utils"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def get_snap_frame(row):\n","    elaped_time_start2snap = row.snap_time - row.start_time\n","    elaped_seconds = elaped_time_start2snap.seconds\n","    snap_frame = elaped_seconds*59.95\n","    return snap_frame"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def set_inimg_window(crop_area, img_size=(720, 1280)):\n","    left, top, right, bot = crop_area\n","    # set crop area (area size in img size)\n","    crop_left = max(0, left)\n","    crop_top = max(0, top)\n","    if crop_left != left:   right = right - left\n","    if crop_top != top:    bot = bot - top\n","    crop_bot = min(img_size[0], bot)\n","    crop_right = min(img_size[1], right)\n","    if crop_bot != bot:    crop_top = crop_top - (bot - img_size[0])\n","    if crop_right != right:   crop_left = left - (right - img_size[1])\n","    \n","    return [crop_left, crop_top, crop_right, crop_bot]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def make_player_mask(img, helmet_pos, img_size=(720, 1280, 3), alpha=0.3):\n","    crop_size=[-helmet_pos[2], -helmet_pos[3], helmet_pos[2]*3, helmet_pos[3]*6] # helmetの大きさによってplayerの範囲も変更\n","    base_area = np.array(helmet_pos) + np.array(crop_size) # [left, top, width, height]\n","    # set players area\n","    palyer_area = [base_area[0],  base_area[1], base_area[0] + base_area[2], base_area[1] + base_area[3]]\n","    palyer_area = set_inimg_window(palyer_area)\n","    mask = np.zeros(img_size, dtype=np.uint8)\n","    cv2.rectangle(mask, [palyer_area[0], palyer_area[1]], [palyer_area[2], palyer_area[3]], (255, 255, 255), -1)\n","    mask = np.clip(mask, 0, 1).astype(np.uint8)\n","    return mask"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def get_1player_croparea(helmet_pos, img_size=(720, 1280)):\n","    crop_size=[-helmet_pos[2]*3, -helmet_pos[3]*4, helmet_pos[2]*6, helmet_pos[3]*6] # helmetの大きさによってplayerの範囲も変更\n","    players_area = np.array(helmet_pos) + np.array(crop_size) # [left, top, width, height]\n","    # set players area\n","    crop_area = [players_area[0],  players_area[1], players_area[0] + players_area[2], players_area[1] + players_area[3]]\n","    crop_area = set_inimg_window(crop_area)\n","    return crop_area"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def get_2player_croparea(helmet1_pos, helmet2_pos, img_size=(720, 1280)):\n","    player1_crop_size=[-10, -10, helmet1_pos[2]*4, helmet1_pos[3]*4] # helmetの大きさによってplayerの範囲も変更\n","    player2_crop_size=[-10, -10, helmet2_pos[2]*4, helmet2_pos[3]*4] # helmetの大きさによってplayerの範囲も変更\n","    player1_area = np.array(helmet1_pos) + np.array(player1_crop_size) # [left, top, width, height]\n","    player2_area = np.array(helmet2_pos) + np.array(player2_crop_size) # [left, top, width, height]\n","    # [left, top, width, height] => [left, top, right, bot]\n","    player1_crop_area = np.array([player1_area[0], player1_area[1], player1_area[0]+player1_area[2], player1_area[1]+player1_area[3]]) # [left, top, right, bot]\n","    player2_crop_area = np.array([player2_area[0], player2_area[1], player2_area[0]+player2_area[2], player2_area[1]+player2_area[3]]) # [left, top, right, bot]\n","    # get min max for set pos in img\n","    area_min = np.min(np.array([player1_crop_area, player2_crop_area]), axis=0)\n","    area_max = np.max(np.array([player1_crop_area, player2_crop_area]), axis=0)\n","    crop_area = [area_min[0], area_min[1], area_max[2], area_max[3]]\n","    crop_area = set_inimg_window(crop_area)\n","    return crop_area"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def mask_blend(img, mask):\n","    img = np.clip(img, 0, 255).astype(np.uint8)\n","    mask = np.clip(mask, 0, 1).astype(np.uint8)\n","    img = img*mask\n","    img = np.clip(img, 0, 255).astype(np.uint8)\n","    return img"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def search_helmet(helmet_df, player, frame):\n","    for diff_frame in range(5):\n","        read_frame = frame + diff_frame\n","        player_helmet = helmet_df.query('nfl_player_id==@player and frame==@read_frame')\n","        if len(player_helmet) > 0:\n","            return player_helmet\n","        read_frame = frame - diff_frame\n","        player_helmet = helmet_df.query('nfl_player_id==@player and frame==@read_frame')\n","        if len(player_helmet) > 0:\n","            return player_helmet\n","    return []"]},{"cell_type":"markdown","metadata":{},"source":["# Read Data"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T13:46:28.602228Z","iopub.status.busy":"2022-12-11T13:46:28.601756Z","iopub.status.idle":"2022-12-11T13:46:56.551352Z","shell.execute_reply":"2022-12-11T13:46:56.550245Z","shell.execute_reply.started":"2022-12-11T13:46:28.602189Z"},"trusted":true},"outputs":[],"source":["helmet_df = pd.read_csv(CFG.TRAIN_HELMET_CSV)\n","tracking_df = pd.read_csv(CFG.TRAIN_TRACKING_CSV)\n","videometa_df = pd.read_csv(CFG.TRAIN_VIDEO_META_CSV, parse_dates=[\"start_time\", \"end_time\", \"snap_time\"])\n","target_df = pd.read_csv(CFG.TRAIN_LABEL_CSV, parse_dates=[\"datetime\"])"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["videometa_df[\"snap_frame\"] = videometa_df.apply(get_snap_frame, axis=1)\n","helmet_df = helmet_df[['game_play', 'view', 'frame', 'nfl_player_id', 'left', 'width', 'top', 'height']]"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["all game play num =  240\n","10 3\n"]}],"source":["print(\"all game play num = \" ,len(target_df[\"game_play\"].unique()))\n","\n","if not DEBUG:\n","    train_gameplays = target_df[\"game_play\"].unique()[:50]\n","    valid_gameplays = target_df[\"game_play\"].unique()[-10:]\n","else:\n","    train_gameplays = target_df[\"game_play\"].unique()[:10]\n","    valid_gameplays = target_df[\"game_play\"].unique()[-3:]\n","\n","print(len(train_gameplays), len(valid_gameplays))\n","train_videos = [[gameplay,\"Endzone\"] for gameplay in train_gameplays]\n","valid_videos = [[gameplay,\"Endzone\"] for gameplay in valid_gameplays]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["class NFLDataset(Dataset):\n","    def __init__(self, game_play, view, target_df, helmet_df, meta_df, transform=None):\n","        self.target_df = target_df\n","        self.helmet_df = helmet_df\n","        self.meta_df = meta_df\n","        self.transform = transform\n","        self.game_play = game_play\n","        video_file = game_play + \"_\" + view + \".mp4\"\n","        self.view = view\n","        self.video_path = os.path.join(CFG.BASE_DIR, \"train\", video_file)\n","        self.cam = cv2.VideoCapture(self.video_path)\n","\n","    def __len__(self):\n","        return len(self.target_df)\n","\n","    def __getitem__(self, idx):\n","        target_info = self.target_df.iloc[idx]\n","        target = target_info.contact\n","        game_play1, game_play2, step, player1, player2 = target_info.contact_id.split(\"_\")\n","        # game_play = game_play1 + \"_\" + game_play2\n","        # view = \"Endzone\"\n","        meta_info = self.meta_df.query('game_play==@self.game_play and view==@self.view')\n","        snap_frame = int(meta_info.snap_frame)\n","        read_frame = snap_frame + int(step) \n","        # print(read_frame)\n","        self.cam.set(cv2.CAP_PROP_POS_FRAMES, int(read_frame))\n","        ret, img = self.cam.read()\n","        if ret:\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","            img = np.clip(img, 0, 255).astype(np.uint8)\n","            player1 = int(player1)\n","            helmet_info = self.helmet_df.query('game_play==@self.game_play and view==@self.view')\n","            # player1_helmet = helmet_info.query('nfl_player_id==@player1 and frame==@read_frame')\n","            player1_helmet = search_helmet(helmet_info, player1, read_frame)\n","            if player2 == \"G\":\n","                if len(player1_helmet) > 0:\n","                    # print(\"only player1 p2=G\")\n","                    helmet_pos = [player1_helmet.left.values[0], player1_helmet.top.values[0],\n","                                  player1_helmet.width.values[0], player1_helmet.height.values[0]]\n","                    crop_area = get_1player_croparea(helmet_pos)\n","                    mask = make_player_mask(img, helmet_pos)\n","                    img = mask_blend(img, mask)\n","                    img = img[crop_area[1]:crop_area[3], crop_area[0]:crop_area[2], :]\n","                else:\n","                    # print(\"no player p2=G\")\n","                    pass\n","            else:\n","                player2 = int(player2)\n","                # player2_helmet = helmet_info.query('nfl_player_id==@player2 and frame==@read_frame')\n","                player2_helmet = search_helmet(helmet_info, player2, read_frame)\n","                if len(player2_helmet) == 0 and len(player1_helmet)==0:\n","                    # print(\"no player len0 p2 not G\")\n","                    pass\n","                elif len(player1_helmet) > 0 and len(player2_helmet) == 0:\n","                    # print(\"only player1 p2notG\")\n","                    helmet_pos = [player1_helmet.left.values[0], player1_helmet.top.values[0],\n","                                  player1_helmet.width.values[0], player1_helmet.height.values[0]]\n","                    # img = make_player_mask(img, helmet_pos)\n","                    mask = make_player_mask(img, helmet_pos)\n","                    img = mask_blend(img, mask)\n","                    crop_area = get_1player_croparea(helmet_pos)\n","                    img = img[crop_area[1]:crop_area[3], crop_area[0]:crop_area[2], :]                    \n","                elif len(player2_helmet) > 0 and len(player1_helmet) == 0:\n","                    # print(\"only player2\")\n","                    helmet_pos = [player2_helmet.left.values[0], player2_helmet.top.values[0],\n","                                  player2_helmet.width.values[0], player2_helmet.height.values[0]]\n","                    # img = make_player_mask(img, helmet_pos)\n","                    mask = make_player_mask(img, helmet_pos)\n","                    img = mask_blend(img, mask)\n","                    crop_area = get_1player_croparea(helmet_pos)\n","                    img = img[crop_area[1]:crop_area[3], crop_area[0]:crop_area[2], :]\n","                else:\n","                    # print(\"two players\")\n","                    helmet1_pos = [player1_helmet.left.values[0], player1_helmet.top.values[0],\n","                                  player1_helmet.width.values[0], player1_helmet.height.values[0]]\n","                    helmet2_pos = [player2_helmet.left.values[0], player2_helmet.top.values[0],\n","                                  player2_helmet.width.values[0], player2_helmet.height.values[0]]\n","                    mask1 = make_player_mask(img, helmet1_pos)\n","                    mask2 = make_player_mask(img, helmet2_pos)\n","                    mask = mask1 + mask2\n","                    img = mask_blend(img, mask)\n","                    crop_area = get_2player_croparea(helmet1_pos, helmet2_pos)\n","                    img = img[crop_area[1]:crop_area[3], crop_area[0]:crop_area[2], :]\n","\n","            img = cv2.resize(img, dsize=CFG.img_size)\n","            img = img / 255. # convert to 0-1\n","            img = np.transpose(img, (2, 0, 1)).astype(np.float32)\n","            img = torch.tensor(img, dtype=torch.float)\n","            target = torch.tensor(target, dtype=torch.float)\n","            return img, target\n","        else:\n","            img = np.zeros(CFG.img_size)\n","            img = np.transpose(img, (2, 0, 1)).astype(np.float32)\n","            img = torch.tensor(img, dtype=torch.float)\n","            target = torch.tensor(target, dtype=torch.float)\n","            return img, target"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# without meta\n","class NFLNet(nn.Module):\n","    def __init__(\n","        self,\n","        model_name = CFG.model_name,\n","        out_features = CFG.out_features,\n","        inp_channels= CFG.inp_channels,\n","        pretrained = CFG.pretrained\n","    ):\n","        super().__init__()\n","        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels, num_classes = out_features)\n","    \n","    def forward(self, image):\n","        output = self.model(image)\n","        return output"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Loss"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# criterion = nn.BCEWithLogitsLoss() #define in train loop"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# scheduler"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# def divice_norm_bias(model): \n","#     norm_bias_params = []\n","#     non_norm_bias_params = []\n","#     except_wd_layers = ['norm', '.bias']\n","#     for n, p in model.model.named_parameters():\n","#         if any([nd in n for nd in except_wd_layers]):\n","#             norm_bias_params.append(p)\n","#         else:\n","#             non_norm_bias_params.append(p)\n","#     return norm_bias_params, non_norm_bias_params"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# def get_scheduler(optimizer):\n","#     scheduler = OneCycleLR(\n","#         optimizer,\n","#         max_lr=CFG.max_lr,\n","#         pct_start = 0.25, # same as fastai, defaut 0.3\n","#         steps_per_epoch=int(((CFG.n_fold - 1) * target_df.shape[0]) / (CFG.n_fold * CFG.batch_size)) + 1,\n","#         epochs = CFG.n_epoch\n","#     )\n","#     return scheduler"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# valid fn"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# def valid_fn(model, criterion, valid_videos):\n","#     model.eval()# モデルを検証モードに設定\n","    \n","#     valid_dataset = \"\"\n","#     for game_play, view in valid_videos:\n","#         target_game = target_df.query('game_play==@game_play')\n","#         helmet_game = helmet_df.query('game_play==@game_play and view==@view')\n","#         videometa_game = videometa_df.query('game_play==@game_play and view==@view')\n","#         valid_dataset_tmp = NFLDataset(game_play, view, target_game, helmet_game, videometa_game)\n","#         if len(valid_dataset) == 0:\n","#             valid_dataset = valid_dataset_tmp\n","#         else:\n","#             valid_dataset += valid_dataset_tmp\n","            \n","#     valid_loader = DataLoader(\n","#         valid_dataset,\n","#         batch_size = CFG.batch_size,\n","#         shuffle = True,\n","#         num_workers = CFG.num_workers,\n","#         pin_memory = True\n","#     )\n","    \n","#     test_targets = []\n","#     test_preds = []\n","#     for i, (images, targets) in tqdm(enumerate(valid_loader)):\n","#         images = images.to(device, non_blocking = True).float()\n","#         targets = targets.to(device, non_blocking = True).float().view(-1, 1)\n","#         with torch.no_grad():\n","#             preds = model(images)\n","#         loss = criterion(preds, targets)\n","#         # score = matthews_corrcoef(preds, targets)\n","\n","#         targets = (targets.detach().cpu().numpy()).ravel().tolist()\n","#         preds = (torch.sigmoid(preds).detach().cpu().numpy()).ravel().tolist()\n","\n","#         test_preds.extend(pred)\n","#         test_targets.extend(targets)\n","    \n","#     test_preds = np.array(test_preds)\n","#     test_targets = np.array(test_targets)\n","#     del valid_loader, valid_dataset, target, pred\n","#     gc.collect()\n","#     torch.cuda.empty_cache()\n","#     return test_targets, test_preds"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def valid_fn(model, criterion, valid_videos):\n","    model.eval()# モデルを検証モードに設定\n","    test_targets = []\n","    test_preds = []\n","    \n","    for game_play, view in valid_videos:\n","        target_game = target_df.query('game_play==@game_play')\n","        helmet_game = helmet_df.query('game_play==@game_play and view==@view')\n","        videometa_game = videometa_df.query('game_play==@game_play and view==@view')\n","        valid_dataset = NFLDataset(game_play, view, target_game, helmet_game, videometa_game)\n","            \n","        valid_loader = DataLoader(\n","            valid_dataset,\n","            batch_size = CFG.batch_size,\n","            shuffle = True,\n","            num_workers = CFG.num_workers,\n","            pin_memory = True\n","        )\n","\n","        for i, (images, targets) in tqdm(enumerate(valid_loader)):\n","            images = images.to(device, non_blocking = True).float()\n","            targets = targets.to(device, non_blocking = True).int().view(-1, 1)\n","            with torch.no_grad():\n","                preds = model(images)\n","            loss = criterion(preds, targets)\n","            # score = matthews_corrcoef(preds, targets)\n","\n","            targets = targets.detach().cpu().numpy().ravel().tolist()\n","            preds = torch.sigmoid(preds).detach().cpu().numpy().ravel().tolist()\n","\n","            test_preds.extend(pred)\n","            test_targets.extend(targets)\n","        \n","        test_preds = np.array(test_preds)\n","        test_targets = np.array(test_targets)\n","        del valid_loader, valid_dataset, target, pred\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","    return test_targets, test_preds"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Train Loop"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# def training_loop(train_videos, valid_videos):\n","#     # oof_df = pd.DataFrame()\n","#     \"\"\" \n","#     Set dataset\n","#     \"\"\"\n","#     train_dataset = \"\"\n","#     for game_play, view in train_videos:\n","#         target_game = target_df.query('game_play==@game_play')\n","#         helmet_game = helmet_df.query('game_play==@game_play and view==@view')\n","#         videometa_game = videometa_df.query('game_play==@game_play and view==@view')\n","#         train_dataset_tmp = NFLDataset(game_play, view, target_game, helmet_game, videometa_game)\n","#         if len(train_dataset) == 0:\n","#             train_dataset = train_dataset_tmp\n","#         else:\n","#             train_dataset += train_dataset_tmp\n","\n","#     train_loader = DataLoader(\n","#         train_dataset,\n","#         batch_size = CFG.batch_size,\n","#         shuffle = True,\n","#         num_workers = CFG.num_workers,\n","#         pin_memory = True\n","#     )\n","#     \"\"\"\n","#     instantiate model, cost function and optimizer\n","#     \"\"\"\n","#     model = NFLNet()\n","#     model = model.to(device)\n","#     criterion = nn.BCEWithLogitsLoss()\n","#     norm_bias_params, non_norm_bias_params = divice_norm_bias(model)\n","#     optimizer = torch.optim.AdamW(\n","#       [\n","#           {'params': norm_bias_params, 'weight_decay': CFG.opt_wd_norm_bias},\n","#           {'params': non_norm_bias_params, 'weight_decay': CFG.opt_wd_non_norm_bias},\n","#       ],\n","#       betas=(CFG.opt_beta1, CFG.opt_beta2),\n","#       eps=CFG.opt_eps,\n","#       lr = CFG.lr,\n","#       amsgrad = False\n","#     )\n","#     scheduler = get_scheduler(optimizer)\n","#     \"\"\"\n","#     train / valid loop\n","#     \"\"\"\n","#     best_rmse = np.inf\n","#     scaler = GradScaler()\n","    \n","#     for epoch in range(1, CFG.n_epoch + 1):\n","#         print(f'=== epoch: {epoch}: training ===')\n","        \n","#         for batch_idx, (images, targets) in enumerate(train_loader):\n","#             model.train()# modelを学習するモードに設定\n","#             # 入力データ(image)とラベルデータ(target)をGPUへ\n","#             images = images.to(device, non_blocking = True).float()\n","#             targets = targets.to(device, non_blocking = True).float().view(-1, 1)\n","#             optimizer.zero_grad()\n","            \n","#             with autocast(): # mixed precision\n","#                 preds = model(images)\n","#                 loss = criterion(preds, targets)\n","#             score = matthews_corrcoef(preds, targets)\n","            \n","#             # 学習率の更新\n","#             scaler.scale(loss).backward()\n","#             scaler.step(optimizer)\n","#             scaler.update()\n","#             scheduler.step()\n","            \n","#             # 設定したepochでvalidation(検証)データでの予測精度を確認する\n","#             if ( ( ( batch_idx % CFG.steps_per_epoch == 0) & (epoch >= CFG.epoch_step_valid) ) | ( batch_idx == len(train_loader) ) ):\n","#                 valid_targets, preds = valid_fn(model, criterion, valid_videos)\n","#                 valid_score = matthews_corrcoef(preds, targets)\n","#                 print(f'epoch: {epoch}, batch: {batch_idx}/{len(train_loader)}, valid rmse: {valid_score}')\n","#                 scheduler.step(valid_score)\n","                \n","#                 # validationスコアがbestを更新したらモデルを保存する\n","#                 if valid_score > best_score:\n","#                     best_score = valid_score\n","#                     model_name = CFG.model_path\n","#                     # torch.save(model.state_dict(), f'{CFG.model_dir}/{model_name}_fold{i_fold}.pth')\n","#                     torch.save(model.state_dict(), f'{CFG.model_dir}/{model_name}.pth')\n","#                     print(\"saved model.\")\n","#                     # _oof_df = pd.DataFrame(data={'Id': valid_ids, 'pred':preds, 'fold': i_fold, 'Pawpularity':valid_targets}, index=valid_idx)\n","\n","#     del model, output, train_loader, train_dataset\n","#     gc.collect()\n","    \n","#     torch.cuda.empty_cache()\n","#     # oof_df = pd.concat([oof_df, _oof_df])\n","#     # return oof_df.sort_values('Id')"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def training_loop(train_videos, valid_videos):\n","    # oof_df = pd.DataFrame()\n","    \"\"\"\n","    instantiate model, cost function and optimizer\n","    \"\"\"\n","    model = NFLNet()\n","    model = model.to(device)\n","    criterion = nn.BCEWithLogitsLoss()\n","    # norm_bias_params, non_norm_bias_params = divice_norm_bias(model)\n","    optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay, amsgrad=False)\n","    scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n","    \n","    \"\"\" \n","    Set dataset\n","    \"\"\"\n","    # train_dataset = \"\"\n","    for game_play, view in train_videos:\n","        print(f\"game_play={game_play}, view={view}\")\n","        target_game = target_df.query('game_play==@game_play')\n","        helmet_game = helmet_df.query('game_play==@game_play and view==@view')\n","        videometa_game = videometa_df.query('game_play==@game_play and view==@view')\n","        train_dataset = NFLDataset(game_play, view, target_game, helmet_game, videometa_game)\n","\n","        train_loader = DataLoader(\n","            train_dataset,\n","            batch_size = CFG.batch_size,\n","            shuffle = True,\n","            num_workers = CFG.num_workers,\n","            pin_memory = True\n","        )\n","\n","        \"\"\"\n","        train / valid loop\n","        \"\"\"\n","        best_score = -np.inf\n","        scaler = GradScaler()\n","        \n","        for epoch in range(1, CFG.n_epoch + 1):\n","            print(f'=== epoch: {epoch}: training ===')\n","            \n","            for batch_idx, (images, targets) in tqdm(enumerate(train_loader), total=len(train_loader)):\n","                model.train()# modelを学習するモードに設定\n","                # 入力データ(image)とラベルデータ(target)をGPUへ\n","                images = images.to(device, non_blocking = True).float()\n","                targets = targets.to(device, non_blocking = True).float().view(-1, 1)\n","                optimizer.zero_grad()\n","                \n","                with autocast(): # mixed precision\n","                    preds = model(images)\n","                    loss = criterion(preds, targets)\n","                    \n","                targets = targets.detach().cpu().numpy().ravel().tolist()\n","                preds = torch.sigmoid(preds).detach().cpu().numpy().ravel().tolist()\n","                targets = (np.array(targets) > 0.5).astype(int)\n","                preds = (np.array(preds) > 0.5).astype(int)\n","                # score = matthews_corrcoef(torch.sigmoid(preds[:,0]), torch.tensor(targets[:,0], dtype=torch.int))\n","                score = matthews_corrcoef(targets, preds) # sklearn\n","\n","                \n","                # 学習率の更新\n","                scaler.scale(loss).backward()\n","                scaler.step(optimizer)\n","                scaler.update()\n","                scheduler.step()\n","                \n","                # 設定したepochでvalidation(検証)データでの予測精度を確認する\n","                if ( ( ( batch_idx % CFG.steps_per_epoch == 0) & (epoch >= CFG.epoch_step_valid) ) | ( batch_idx == len(train_loader) ) ):\n","                    valid_targets, valid_preds = valid_fn(model, criterion, valid_videos)\n","                    # valid_score = matthews_corrcoef(valid_preds, valid_targets)\n","                    valid_score = matthews_corrcoef(valid_targets, valid_preds)\n","                    print(f'epoch: {epoch}, batch: {batch_idx}/{len(train_loader)}, valid score: {valid_score}')\n","                    scheduler.step(valid_score)\n","                    \n","                    # validationスコアがbestを更新したらモデルを保存する\n","                    if valid_score > best_score:\n","                        best_score = valid_score\n","                        model_name = CFG.model_path\n","                        # torch.save(model.state_dict(), f'{CFG.model_dir}/{model_name}_fold{i_fold}.pth')\n","                        torch.save(model.state_dict(), f'{CFG.model_dir}/{model_name}.pth')\n","                        print(\"saved model.\")\n","                        # _oof_df = pd.DataFrame(data={'Id': valid_ids, 'pred':preds, 'fold': i_fold, 'Pawpularity':valid_targets}, index=valid_idx)\n","\n","        del model, output, train_loader, train_dataset\n","        gc.collect()\n","        \n","        torch.cuda.empty_cache()\n","        # oof_df = pd.concat([oof_df, _oof_df])\n","        # return oof_df.sort_values('Id')"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["game_play=58168_003392, view=Endzone\n","=== epoch: 1: training ===\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f16b4c7f758246a5b741bb2151685119","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/70 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31907/1707840905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_videos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_videos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_31907/3188552285.py\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(train_videos, valid_videos)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'=== epoch: {epoch}: training ==='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# modelを学習するモードに設定\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;31m# 入力データ(image)とラベルデータ(target)をGPUへ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31907/31819096.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mread_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnap_frame\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# print(read_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_POS_FRAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["training_loop(train_videos, valid_videos)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for game_play, view in train_videos:\n","#         print(f\"game_play={game_play}, view={view}\")\n","#         target_game = target_df.query('game_play==@game_play')\n","#         helmet_game = helmet_df.query('game_play==@game_play and view==@view')\n","#         videometa_game = videometa_df.query('game_play==@game_play and view==@view')\n","#         train_dataset = NFLDataset(game_play, view, target_game, helmet_game, videometa_game)\n","\n","#         train_loader = DataLoader(\n","#             train_dataset,\n","#             batch_size = CFG.batch_size,\n","#             shuffle = True,\n","#             num_workers = CFG.num_workers,\n","#             pin_memory = True\n","#         )        \n","#         for epoch in range(1, CFG.n_epoch + 1):\n","#             print(f'=== epoch: {epoch}: training ===')\n","            \n","#             for batch_idx, (images, targets) in enumerate(train_loader):\n","#                 for idx in range(CFG.batch_size):\n","#                     plt.imshow(images[idx])\n","#                     plt.title(f\"target={targets[idx]}\")\n","#                     plt.show()\n","#             break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"vscode":{"interpreter":{"hash":"d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"}}},"nbformat":4,"nbformat_minor":4}
